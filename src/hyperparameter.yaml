models:
  DecisionTreeRegressor:
    criterion: ["squared_error", "friedman_mse", "absolute_error", "poisson"]
    splitter: ["best", "random"]
    max_depth: [null, 5, 10, 20, 30]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    max_features: [null, "sqrt", "log2"]

  LinearRegression:
    fit_intercept: [true, false]
    copy_X: [true, false]

  KNeighborsRegressor:
    n_neighbors: [3, 5, 7, 9, 11]
    weights: ["uniform", "distance"]
    algorithm: ["auto", "ball_tree", "kd_tree", "brute"]
    leaf_size: [20, 30, 40]
    p: [1, 2]

  RandomForestRegressor:
    n_estimators: [100, 200, 500]
    criterion: ["squared_error", "friedman_mse", "absolute_error", "poisson"]
    max_depth: [null, 5, 10, 20]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    max_features: ["sqrt", "log2",null]

  AdaBoostRegressor:
    n_estimators: [50, 100, 200]
    learning_rate: [0.01, 0.05, 0.1, 0.5, 1.0]
    loss: ["linear", "square", "exponential"]

  GradientBoostingRegressor:
    n_estimators: [100, 200, 300]
    learning_rate: [0.01, 0.05, 0.1, 0.2]
    max_depth: [3, 5, 7, 10]
    min_samples_split: [2, 5, 10]
    min_samples_leaf: [1, 2, 4]
    subsample: [0.6, 0.8, 1.0]
    max_features: ["sqrt", "log2",null]

  XGBRegressor:
    n_estimators: [100, 200, 500]
    learning_rate: [0.01, 0.05, 0.1, 0.2]
    max_depth: [3, 5, 7, 10]
    min_child_weight: [1, 3, 5]
    gamma: [0, 0.1, 0.3, 0.5]
    subsample: [0.6, 0.8, 1.0]
    colsample_bytree: [0.6, 0.8, 1.0]
    reg_alpha: [0, 0.01, 0.1, 1]
    reg_lambda: [1, 1.5, 2]
